{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 — Modelling\n",
                "\n",
                "Train and evaluate six regression models on the London Airbnb dataset:\n",
                "\n",
                "| # | Model | Type |\n",
                "|---|---|---|\n",
                "| 1 | Linear Regression | Baseline |\n",
                "| 2 | Ridge Regression | L2 Regularisation |\n",
                "| 3 | Lasso Regression | L1 Regularisation |\n",
                "| 4 | Random Forest | Ensemble (bagging) |\n",
                "| 5 | XGBoost | Gradient Boosting |\n",
                "| 6 | CatBoost | Gradient Boosting |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from src.data_loader import load_listings, load_reviews\n",
                "from src.data_cleaning import clean_listings\n",
                "from src.feature_engineering import engineer_features\n",
                "from src.modeling import (\n",
                "    prepare_features, split_data,\n",
                "    train_linear_regression, train_ridge, train_lasso,\n",
                "    train_random_forest, train_xgboost, train_catboost,\n",
                "    evaluate_model, compare_models, run_kfold_cv,\n",
                "    get_feature_importance,\n",
                ")\n",
                "from src.visualization import (\n",
                "    set_style,\n",
                "    plot_predictions_vs_actual,\n",
                "    plot_feature_importance,\n",
                "    plot_regularization_trace,\n",
                "    plot_model_comparison,\n",
                ")\n",
                "\n",
                "set_style()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Prepare Data\n",
                "\n",
                "Load, clean, engineer features, then split into train/test sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and process\n",
                "listings_raw = load_listings()  # Uses DEFAULT_CITY from config\n",
                "listings = clean_listings(listings_raw)\n",
                "\n",
                "# If you have pre-computed sentiment, merge it here.\n",
                "# Otherwise, run notebook 02 first to generate sentiment scores.\n",
                "\n",
                "listings = engineer_features(listings)  # Uses DEFAULT_CITY from config\n",
                "print(f'Feature matrix: {listings.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y, scaler = prepare_features(listings)\n",
                "X_train, X_test, y_train, y_test = split_data(X, y)\n",
                "\n",
                "print(f'Training set: {X_train.shape[0]:,} samples')\n",
                "print(f'Test set:     {X_test.shape[0]:,} samples')\n",
                "print(f'Features:     {X_train.shape[1]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Linear Regression (Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_model = train_linear_regression(X_train, y_train)\n",
                "lr_results = evaluate_model(lr_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'Linear Regression — Test R²: {lr_results[\"test_r2\"]}, Test MSE: {lr_results[\"test_mse\"]}')\n",
                "plot_predictions_vs_actual(y_train, lr_results['train_pred'], y_test, lr_results['test_pred'],\n",
                "                           title='Linear Regression')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Ridge Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge_model, ridge_trace = train_ridge(X_train, y_train)\n",
                "ridge_results = evaluate_model(ridge_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'Ridge — Test R²: {ridge_results[\"test_r2\"]}, Test MSE: {ridge_results[\"test_mse\"]}')\n",
                "plot_regularization_trace(ridge_trace, title='Ridge Regression Trace')\n",
                "plot_predictions_vs_actual(y_train, ridge_results['train_pred'], y_test, ridge_results['test_pred'],\n",
                "                           title='Ridge Regression')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Lasso Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_model, lasso_trace = train_lasso(X_train, y_train)\n",
                "lasso_results = evaluate_model(lasso_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'Lasso — Test R²: {lasso_results[\"test_r2\"]}, Test MSE: {lasso_results[\"test_mse\"]}')\n",
                "plot_regularization_trace(lasso_trace, title='Lasso Regression Trace')\n",
                "plot_predictions_vs_actual(y_train, lasso_results['train_pred'], y_test, lasso_results['test_pred'],\n",
                "                           title='Lasso Regression')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rf_model = train_random_forest(X_train, y_train)\n",
                "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'Random Forest — Test R²: {rf_results[\"test_r2\"]}, Test MSE: {rf_results[\"test_mse\"]}')\n",
                "\n",
                "rf_importance = get_feature_importance(rf_model, X_train.columns)\n",
                "plot_feature_importance(rf_importance, title='Random Forest — Feature Importance')\n",
                "plot_predictions_vs_actual(y_train, rf_results['train_pred'], y_test, rf_results['test_pred'],\n",
                "                           title='Random Forest')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xgb_model = train_xgboost(X_train, y_train)\n",
                "xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'XGBoost — Test R²: {xgb_results[\"test_r2\"]}, Test MSE: {xgb_results[\"test_mse\"]}')\n",
                "\n",
                "xgb_importance = get_feature_importance(xgb_model, X_train.columns)\n",
                "plot_feature_importance(xgb_importance, title='XGBoost — Feature Importance')\n",
                "plot_predictions_vs_actual(y_train, xgb_results['train_pred'], y_test, xgb_results['test_pred'],\n",
                "                           title='XGBoost')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. CatBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "catb_model = train_catboost(X_train, y_train)\n",
                "catb_results = evaluate_model(catb_model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "print(f'CatBoost — Test R²: {catb_results[\"test_r2\"]}, Test MSE: {catb_results[\"test_mse\"]}')\n",
                "\n",
                "catb_importance = get_feature_importance(catb_model, X_train.columns)\n",
                "plot_feature_importance(catb_importance, title='CatBoost — Feature Importance')\n",
                "plot_predictions_vs_actual(y_train, catb_results['train_pred'], y_test, catb_results['test_pred'],\n",
                "                           title='CatBoost')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results = {\n",
                "    'Linear Regression': lr_results,\n",
                "    'Ridge':             ridge_results,\n",
                "    'Lasso':             lasso_results,\n",
                "    'Random Forest':     rf_results,\n",
                "    'XGBoost':           xgb_results,\n",
                "    'CatBoost':          catb_results,\n",
                "}\n",
                "\n",
                "comparison = compare_models(all_results)\n",
                "display(comparison)\n",
                "\n",
                "plot_model_comparison(comparison, metric='Test R²')\n",
                "plot_model_comparison(comparison, metric='Test MSE')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. K-Fold Cross Validation (10-Fold)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cv_models = {\n",
                "    'Linear Regression': lr_model,\n",
                "    'Ridge':             ridge_model,\n",
                "    'Lasso':             lasso_model,\n",
                "    'Random Forest':     rf_model,\n",
                "    'XGBoost':           xgb_model,\n",
                "    'CatBoost':          catb_model,\n",
                "}\n",
                "\n",
                "cv_results = run_kfold_cv(cv_models, X, y, k=10)\n",
                "display(cv_results)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}